An extreme version of gradient descent is to use a mini-batch size of just 1. That is, given a training input, $`x`$, we update our weights and biases according to the rules $`w_k \rightarrow w_k' = w_k - \eta \partial C_x / \partial w_k`$ and $`b_l \rightarrow b_l' =  b_l - \eta \partial C_x / \partial b_l`$. Then we choose another training input, and update the weights and biases again. And so on, repeatedly. This procedure is known as online, on-line, or incremental learning. In online learning, a neural network learns from just one training input at a time (just as human beings do). Name one advantage and one disadvantage of online learning, compared to stochastic gradient descent with a mini-batch size of, say, 20.